from pathlib import Path
from data_loader import load_raw_data
from cleaning_steps import handle_outliers, handle_missing_values, filter_blacklist

def run_pipeline():
    """
    æ‰§è¡Œå®Œæ•´çš„æ•°æ®æ¸…æ´—å·¥ä½œæµ
    ç¡®ä¿æ¸…æ´—åçš„æ•°æ®åŒ…å«ts_codeåˆ—ï¼Œå¹¶æ­£ç¡®å‰”é™¤STè‚¡ç¥¨å’Œæ¬¡æ–°è‚¡
    """
    print("=" * 60)
    print("ğŸš€ å¼€å§‹æ‰§è¡Œè‚¡ç¥¨æ—¥çº¿æ•°æ®æ¸…æ´—æµç¨‹")
    print("=" * 60)
    
    # 1. åŠ è½½å¹¶åˆå¹¶åŸå§‹æ•°æ®
    print("\nğŸ“‚ æ­¥éª¤1: æ•°æ®åŠ è½½")
    df = load_raw_data()
    
    if df is None:
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œæ¸…æ´—æµç¨‹ä¸­æ­¢")
        return
    
    print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {len(df):,} æ¡è®°å½•")
    print(f"ğŸ“Š æ•°æ®åˆ—: {list(df.columns)}")
    
    # ç¡®ä¿å…³é”®åˆ—å­˜åœ¨
    required_cols = ['ts_code', 'name', 'trade_date', 'list_date', 'pct_chg']
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        print(f"âŒ ç¼ºå°‘å…³é”®åˆ—: {missing_cols}")
        return

    # 2. ä¾æ¬¡æ‰§è¡Œæ¸…æ´—æ­¥éª¤
    print("\nğŸ§¹ æ­¥éª¤2: æ•°æ®æ¸…æ´—")
    
    # 2a. å¤„ç†ç¦»ç¾¤å€¼
    print("\n--- 2a. å¤„ç†ç¦»ç¾¤å€¼ ---")
    df = handle_outliers(df)
    
    # 2b. å¤„ç†ç¼ºå¤±å€¼
    print("\n--- 2b. å¤„ç†ç¼ºå¤±å€¼ ---") 
    df = handle_missing_values(df)
    
    # 2c. è¿‡æ»¤é»‘åå•è‚¡ç¥¨
    print("\n--- 2c. è¿‡æ»¤é»‘åå•è‚¡ç¥¨ ---")
    df = filter_blacklist(df)
    
    # 3. æ•°æ®è´¨é‡æ£€æŸ¥
    print("\nğŸ” æ­¥éª¤3: æ•°æ®è´¨é‡æ£€æŸ¥")
    print(f"âœ… æœ€ç»ˆæ•°æ®é‡: {len(df):,} æ¡è®°å½•")
    print(f"ğŸ“Š æœ€ç»ˆåˆ—æ•°: {len(df.columns)} åˆ—")
    print(f"ğŸ¢ è‚¡ç¥¨æ•°é‡: {df['ts_code'].nunique():,} åª")
    print(f"ğŸ“… æ—¶é—´è·¨åº¦: {df['trade_date'].min()} ~ {df['trade_date'].max()}")
    
    # éªŒè¯å…³é”®åˆ—
    if 'ts_code' not in df.columns:
        print("âŒ è­¦å‘Š: ts_codeåˆ—ä¸¢å¤±ï¼")
    else:
        print("âœ… ts_codeåˆ—ä¿ç•™å®Œæ•´")
    
    # éªŒè¯STè‚¡ç¥¨å‰”é™¤
    if 'name' in df.columns:
        st_count = df['name'].str.contains('ST', na=False).sum()
        if st_count == 0:
            print("âœ… STè‚¡ç¥¨å·²å®Œå…¨å‰”é™¤")
        else:
            print(f"âš ï¸ ä»æœ‰ {st_count} åªSTè‚¡ç¥¨")
    
    # 4. å­˜å‚¨æ¸…æ´—åçš„æ•°æ®
    print("\nğŸ’¾ æ­¥éª¤4: æ•°æ®å­˜å‚¨")
    clean_data_path = Path(__file__).resolve().parent.parent / 'clean_data'
    clean_data_path.mkdir(parents=True, exist_ok=True)
    save_file = clean_data_path / 'a_stock_daily_data_clean.parquet'
    
    df.to_parquet(save_file, index=False)
    
    print(f"âœ… æ¸…æ´—åæ•°æ®å·²ä¿å­˜è‡³: {save_file}")
    print(f"ğŸ“ æ–‡ä»¶å¤§å°: {save_file.stat().st_size / 1024 / 1024:.1f} MB")
    
    print("\n" + "=" * 60)
    print("ğŸ‰ æ•°æ®æ¸…æ´—æµç¨‹å…¨éƒ¨å®Œæˆï¼")
    print("=" * 60)

if __name__ == '__main__':
    run_pipeline()